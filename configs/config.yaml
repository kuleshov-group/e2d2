#file: noinspection LongLine
defaults:
  - composer: default_composer  # Pull in composer setup defaults
  - tokenizer: autotokenizer
  - collator: denoising
  - dataset@train_dataset: ???
  - dataset@eval_dataset: ???
  - metrics: [loss, nll, bpd, perplexity]
  - model: mdlm
  - sampler@model.config.sampler_config: ancestral
  - noise@model.config.noise_config: linear
  - model/backbone@model.config.backbone_config: ???
  - _self_

run_name: ???
seed: 1
backend: ${set_backend:}
pretrained_model_name_or_path: null  # Use for pretrained backbones
block_size: null  # Use for BD3LMs

train_dataloader:
  _target_: torch.utils.data.DataLoader
  # Note: batch_size is **per device** (this is "macro" batch size if using grad accum)
  batch_size: ${div_up:${training.global_batch_size}, ${get_world_size:}}
  num_workers: ${eval:"len(__import__('os').sched_getaffinity(0))"}
  pin_memory: True

eval_dataloader:
  _target_: torch.utils.data.DataLoader
  # Note: batch_size is **per device** (if grad_accum is used, reduce size to avoid OOM)
  batch_size: ${eval:${div_up:${div_up:${training.eval_global_batch_size}, ${get_world_size:}}, ${training.grad_accum}}*2}
  num_workers: ${train_dataloader.num_workers}
  pin_memory: ${train_dataloader.pin_memory}

training:
  autoresume: true
  load_path: null  # Path to checkpoint from which to load
  ema: 0.9999
  lr: 3e-4  # Peak learning rate
  global_batch_size: 512
  eval_global_batch_size: ${.global_batch_size}
  grad_accum: 1
  # Parameters for sampling t
  antithetic_sampling: True
  sampling_eps: 1e-3
  restricted_t_range: null  # Set to tuple of floats in range [0, 1] to bias sampling

checkpointing:
  # Use custom `save_dir` if, e.g., saving to S3 bucket, otherwise leave this parameter as is
  save_dir: ${cwd:}/checkpoints  # With hydra.job.chdir=true, this will be ${hydra.run.dir}/checkpoints

hydra:
  run:
    dir: ./outputs/${hydra:runtime.choices.dataset@train_dataset}/${now:%Y.%m.%d}/${now:%H%M%S}
  job:
    chdir: true
